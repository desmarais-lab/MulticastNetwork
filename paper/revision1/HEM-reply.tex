\documentclass[12pt]{article}

\topmargin 0pt \advance \topmargin by -\headheight \advance
\topmargin by -\headsep \textheight 8.9in \oddsidemargin 0pt
\evensidemargin \oddsidemargin \marginparwidth 0.5in \textwidth
6.5in

\renewcommand{\baselinestretch}{1.33}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{exscale}
\usepackage{multirow}
\usepackage[mathscr]{eucal}
\usepackage{bm}
\usepackage{eqlist} % Makes for a nice list of symbols.
\usepackage[final]{graphicx}
\usepackage[dvipsnames]{color}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage{caption}
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}


\newcommand{\pjm}[1]{{\color{blue}#1}}
\definecolor{MyGreen}{cmyk}{1.0,0.0,1.0,0.2}
\newcommand{\sjc}[1]{{\color{MyGreen}#1}}
\definecolor{MyFuschia}{cmyk}{0.07,0.95,0,0}
\newcommand{\ejm}[1]{{\color{MyFuschia}#1}}
\definecolor{bloodred}{RGB}{102,0,0}
\newcommand{\blood}[1]{{\color{bloodred}#1}}
\definecolor{mygrey}{gray}{0.45}
\newcommand{\grey}[1]{{\color{mygrey}#1}}
\definecolor{orange}{RGB}{255,102,0}
\newcommand{\orange}[1]{{\color{orange}#1}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\blue}[1]{{\color{blue}#1}}
% The official Dartmouth green: 
\definecolor{dartmouthgreen}{rgb}{0,.41,.24}



\usepackage{etoolbox}

\newtheorem{thm}{Theorem}

\newcommand{\aset}{{\mathcal A}}
\newcommand{\boldbeta}{{\boldsymbol{\beta}}}

\def\red{\color{red}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\def\sgn{\mathrm{sign}}
\def\E{\mathrm{E}}
\def\Var{\mathrm{Var}}
\def\pr{\mathrm{Pr}}
\def\tr{\mathrm{tr}}
\def\ve{\mathrm{vec}}
\def\diag{\mathrm{diag}}

\newcommand{\bff}{\mathrm{\bf f}}
\def\aset{\mathcal{A}}
\def\bset{\mathcal{B}}
\def\sset{\mathcal{S}}
\def\ba{\boldsymbol{a}}
\def\hba{\boldsymbol{\hat a}}
\newcommand{\hf}{\widehat \bff}
\newcommand{\bphi}{\mbox{\boldmath $\phi$}}
\newcommand{\hphi}{\widehat\bphi}
\newcommand{\Sig}{\mathbf{\Sigma}}
\newcommand{\hSig}{\widehat\Sig}
\newcommand{\hbSig}{\widehat\bSigma}
\newcommand{\hlam}{\widehat\lambda}
\newcommand{\hLam}{\widehat \bLam}
\newcommand{\bLam}{\mbox{\boldmath $\Lambda$}}
\def\br{\boldsymbol{r}}
\def\hbr{\boldsymbol{\hat r}}
\def\bu{\boldsymbol{u}}
\def\bv{\boldsymbol{v}}
\def\bx{\boldsymbol{x}}
\def\bxi{\boldsymbol{\xi}}
\def\by{\boldsymbol{y}}
\def\bZ{\boldsymbol{Z}}
\def\bz{\boldsymbol{z}}
\def\bp{\boldsymbol{p}}
\def\bX{\boldsymbol{X}}
\def\bW{\boldsymbol{W}}
\def\bY{\boldsymbol{Y}}
\def\bQ{\boldsymbol{Q}}
\def\hbQ{\boldsymbol{\hat Q}}
\def\bA{\boldsymbol{A}}
\def\hbA{\boldsymbol{\hat A}}
\def\bD{\boldsymbol{D}}
\def\hbD{\boldsymbol{\hat D}}
\def\bI{\boldsymbol{I}}
\def\bS{\boldsymbol{S}}
\def\bV{\boldsymbol{V}}
\def\bR{\boldsymbol{R}}
\def\hbR{\boldsymbol{\hat R}}
\def\tbR{\boldsymbol{\tilde R}}
\def\bH{\boldsymbol{H}}
\def\bL{\boldsymbol{L}}
\def\bM{\boldsymbol{M}}
\def\bN{\boldsymbol{N}}
\def\boldf{\boldsymbol{f}}

\def\bpi{\boldsymbol{\pi}}
\def\bmu{\boldsymbol{\mu}}
\def\hbmu{\boldsymbol{\hat\mu}}
\def\bzero{\boldsymbol{0}}
\def\bSigma{\boldsymbol{\Sigma}}
\def\hbSigma{\boldsymbol{\hat\Sigma}}
\def\bOmega{\boldsymbol{\Omega}}
\def\bPsi{\boldsymbol{\Psi}}
\def\bDelta{\boldsymbol{\Delta}}

\def\bG{\boldsymbol{\Gamma}}
%\def\hbGamma{\boldsymbol{\hat\Gamma}}
%\def\bgamma{\boldsymbol{\gamma}}
%\def\hbgamma{\boldsymbol{\hat\gamma}}

\def\bGamma{\boldsymbol{\Sigma}}
\def\hbGamma{\boldsymbol{\hat\Sigma}}
\def\bgamma{\boldsymbol{\sigma}}
\def\hbgamma{\boldsymbol{\hat\sigma}}

\def\bbeta{\boldsymbol{\beta}}
\def\bpsi{\boldsymbol{\psi}}
\def\btheta{\boldsymbol{\theta}}
\def\mvnorm{N_k(\bzero,\mathbf{\Sigma})}

\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\P{\mathcal{P}}
\def\U{\mathcal{U}}

\def\be{\begin{equation}}
\def\ee{\end{equation}}
\def\ben{\begin{equation*}}
\def\een{\end{equation*}}
\def\bea{\begin{eqnarray}}
\def\eea{\end{eqnarray}}
\def\bean{\begin{eqnarray*}}
\def\eean{\end{eqnarray*}}

\date{\today}
\begin{document}
%\begin{flushright}
%March 31, 2012
%\end{flushright}

\begin{center}
\textsl{The Hyperedge Event Model}
\end{center}
\iffalse
Thank you for acknowledging our efforts and contributions, and also for your constructive suggestions, which are very helpful to improve the quality of our paper. 

\begin{itemize}
	\item Presentation

	\textbf{  Response:} 
	\begin{itemize}
		\item Unusual notations such as actor $A$ and covariates $y$. Also $u_{ie}$ is the $i$th line of matrix $u_e$, while $\tau_e = min_i(\tau_{ie})$.  \textcolor{blue}{(done)}
		\item Change the order of Equation (2.2) and (2.1) and not use `intensity'. \textcolor{blue}{(done)}
		\item Hard to understand 2.2 without 2.3. More explanation on $\tau_{ie}$ above Equation (2.5) and what $\mu$ represents in $V(\mu)$. \textcolor{blue}{(done)}
	\end{itemize}

	\item Discussion of relevant literature

	\textbf{  Response:} 
	\begin{itemize}
		\item Perry and Wolfe (2013) arxiv version has a model for multicast. Any differences/advantages? \textcolor{blue}{(done)}
		\item Why cite Snijders (1996) in 2.3? Be specific. \textcolor{blue}{(done)}
	\end{itemize}
	
	\item Model, covariates and missing data
	
		\textbf{  Response:} 
		\begin{itemize}
			\item Observations $(s_e, r_e, t_e)_{e=1,\ldots,E}$ are not conditionaly independent since covariates depend on last 7 days. State this in Section 2 and modify out-of-sample algorithm using $(s_{e'}, r_{e'}, t_{e'})_{e':t_e<t_e'<t_e+l_e}$.  \textcolor{red}{(discuss)} 
		\end{itemize}
	\item MCMC sampler
	
		\textbf{  Response:} 
		\begin{itemize}
			\item Details on M-H proposals for $\boldsymbol{b}$ and $\boldsymbol{\eta}$ in Section 3.2. \textcolor{blue}{(done)}
			\item Inefficient sampler for $u_{iej}$ especially when most are one-to-one. Comment on this and the mixing of MCMC samplers.  \textcolor{red}{(discuss)} 
			\item Move Geweke to appendix \textcolor{blue}{(done)} and use larger number of nodes and events.
			\item Computational complexity per iterations of the samplers.
		\end{itemize}
		
	\item Typos \textcolor{blue}{(done)}
	
		\textbf{  Response:} We fixed all the typos identified by the reviewer as well as other writing issues, and we highly appreciate your considerable comments on these which were extremely helpful.
\end{itemize}



\begin{center}

\textbf{Authors' Responses to Reviewer 1}
\end{center}
\begin{center}
\textsl{The Hyperedge Event Model}
\end{center}

Thank you for acknowledging our efforts and contributions, and also for your constructive suggestions, which are very helpful to improve the quality of our paper.
\begin{itemize}
	\item Presentation and writing
	
	\textbf{  Response:} We fixed all the typos, unclear parts, and issues in the bibliography \textcolor{blue}{(bib not working)} identified by the reviewer. We highly appreciate your considerable comments on these which were extremely helpful. \textcolor{blue}{(done)}

  \item Literature review

\textbf{  Response:}  We added a subsection following the description of the HEM in which we ground it in the structure of existing models for networks. 
\begin{itemize}
	\item More comprehensive review including temporal ERGMS and dynamic latent variable models, and discuss contributions and novelties in the light of alternatives
\end{itemize}

\item Section 2

\textbf{  Response:} Rewrite Section 2 to provide a much clear picture of the model.


\item Prior specification

\textbf{  Response:} 
\begin{itemize}
	\item Use weakly informative priors as generic priors instead of assuming $N(0, \infty)$  \textcolor{blue}{(done)}
	\item Sensitivity analyses to check how much posterior inference is affected by the hyperparameters' settings, and, possibly, suggest some default values.  \textcolor{red}{(discuss)} 
\end{itemize}

\item Posterior computation  \textcolor{red}{(discuss)} 

\textbf{  Response:} 
\begin{itemize}
	\item Type of MH, proposal distribution, acceptance rate, smart proposal
	\item Comment on poor mixing on data augmentation
	\item Extent of scaling, bigger dataset, information on computational time
\end{itemize}
\item Application  \textcolor{red}{(discuss)} 

\textbf{  Response:} 
\begin{itemize}
	\item Better baseline than random guess $1/18$ 
	\item Compare with SAOMs and extensions in PPE and PPC 
	\item Bad results in predicting timestamps (MdAPE)  \textcolor{blue}{(done)}
	\item More conservative interpretations \textcolor{blue}{(done)}
\end{itemize}
\end{itemize}

\newpage
\begin{center}
	
	\textbf{Revision Plans}
\end{center}
\begin{enumerate}
	\item Section 1: Literature review
	\begin{itemize}
		\item Bomin: add literature review on Perry and Wolfe (2013) arxiv version and comment how our model differs from point process based models (AE1 bullet 2)
		\item Bruce: add literature review on the general class of dynamic network inference including temporal ERGMS and dynamic latent variable models (R1 comment 2)
	\end{itemize}
	\item Section 2: Generative process
		\begin{itemize}
			\item Bomin: change few notations and add notation table in Appendix (AE1 bullet 1)
			\item Bruce: overall rewriting such as rephrasing or clarification (R1 comment 3 \& AE1 bullet 1, minor ones already resolved)
		\end{itemize}
			\item Section 3: Inference
			\begin{itemize}
		\item Bomin: add more MH details and add a subsection 3.2 for computational issues---e.g., complexity and limitations (R1 comment 4, 5 \& AE1 bullet 4)
				\item Bruce: check the added subsection 3.2 and revise
			\end{itemize}
			\item Section 4: Application
			\begin{itemize}
				\item Bomin: re-run PPE considering conditional dependence (section 4.2) and update results \& interpretations in 4.2 \& 4.4 (R1 comment 6 \& AE1 bullet 2)
				\item Bruce: add why direct comparison with SAOMs not possible and come up with better idea than random guess...? (R1 comment 6)
			\end{itemize}			
	
				\item Section 5: Conclusion
				\begin{itemize}
					\item Bruce: possibly further discussing our contribution in the light of alternatives added in the literature review (R1 comment 2)
				\end{itemize}			
					
					\item Bibliography
					\begin{itemize}
						\item Bomin: tons of issues but somehow changes are not reflected...? Double check! (R1 comment 1)
					\end{itemize}	
					
					\item Discuss: sensitivity analysis?					
\end{enumerate}

\fi

\noindent \textbf{Point-by-point discussion of Reviews and Response to Comments}\\

First, we thank the editor for acknowledging our efforts and contributions, and also for your constructive suggestions, which are very helpful to improve the quality of our paper. 

\bigskip
\bigskip


\noindent \underline{\textbf{Response to the Editor's Comments}}\\

\noindent {\bf E1} \grey{\emph{Presentation: I found the presentation rather lacking; in particular, the description of the model in Section 2 is very difficult to follow, due to the numerous variables introduced, the order in which the different elements of the model are introduced, and the lack of definition for some of them. I think a major rewriting is needed here. }}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor pointed out the lack of detailed and coherent explanation on the model in Section 2, suggesting better definition of variables and improvement on writing. We have significantly revised Section 2 by changing the order of subsections along with general rewriting---see E2 to E5 for details. \\

\noindent  {\bf E2} \grey{\emph{Some notations are rather unusual, for example $A$ for nodes. Although this is not indicated in the text, I assume this comes from the term ``actor'' in the stochastic actor-oriented model (SAOM) of Snijders, cited by the author? If this is the case, it would be worth mentioning it so that the reader recalls this later on. $y$ for the covariates is also rather unusual. Some notations are inconsistent. For example $u_{ie}$ denotes the first line of the matrix $u_e$, but $\tau_e = \text{min}_i(\tau_{ie})$. }}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor identified some unusual notations which are not consistent with the literature. Instead of $A$ for nodes (which we got idea from SAOM as the Editor assumed) we now use $V$ for nodes (referring to vertex in network literature), and we also replaced $y$ by $z$ for time-related covariates---which are clearly stated in Section 2. We also added detailed definition on the variables and notations, such as the usage of subscripts $i$, $e$, $j$ right before Section 2.1. \\

\noindent  {\bf E3} \grey{\emph{In section 2.1, I think it makes more sense to First introduce Equation (2.2) then (2.1). The term ``intensity'' used for $\lambda_{iej}$ is rather confusing. As the paper is concerned with a continuous-time model, one would expect that the term intensity refers to some point process, which is not the case here. }}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} Following the Editor's suggestion, we changed the order of Equation (2.2) and (2.1) as well as their following texts to help readers' understandability. Moreover, we agree that the term ``intensity'' could be a confusing term so we revised the sentence withut using the term.\\


\noindent  {\bf E4} \grey{\emph{It is difficult to understand Section 2.2 without reading section 2.3. The authors introduce the notation $\tau_{ie}$ above Equation (2.5), but do not explain what this represents (at this stage I thought it was a component of the vector $\tau_e$). I do not see what the variable $\mu$ represents in Equation (2.5), and if it is related to $\mu_{ie}$ in Equation (2.4)
 }}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor expressed difficulty in understanding Section 2.2 and 2.3, and suggested further clarification on the variables $\tau$ and $\mu$. We moved Fig. 1 to earlier parts of Section 2 and briefly summarized the entire Section 2, such that the readers can understand Section 2.2 without reading Section 2.3. We also further explained the concepts and notations related to timestamp variables, $\tau$ and $\mu$, to avoid any confusion.\\



\noindent  {\bf E5} \grey{\emph{ Some sentences are difficult to understand:
Page 3: ``we define a probability measure ``MBG'' motivated by the Gibbs measure'' }}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} According to the Editor's comment, we revised the sentence mentioned above and also went through entire draft to revise sentences that were not clearly written before. \\


\noindent  {\bf E6} \grey{\emph{ Discussion of relevant literature: The authors should provide a better discussion of how their model differs from other approaches, in particular the approach of Perry and Wolfe (2013), and Snijders (1996), based on point processes. Perry and Wolfe propose a specific model for multicast interaction (Equation (6) in the arxiv version of their paper). What are the differences/advantages between both constructions? In Section 2.3, the author cite Snijders (1996) when they introduce the model for the interaction arrival times. I am not sure what is meant here: is this part of the model already introduced in Snijders (1996)? The authors should be more specific here.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor... \\

\noindent  {\bf E7} \grey{\emph{ Model, covariates and missing data: This is not explicitly mentioned in the definition of the model in Section 2, but the covariates $x_e$ and $y_e$ depend on the observed interaction data ($s_{e'}$ , $r_{e'}$ , $t_{e'}$ ) in the last 7 days, as written in Section 4.1. Hence the observations ($s_e$, $r_e$, $t_e$)$_{e=1,...,E}$ are not conditionally independent given the model parameters. This should be clearly stated in Section 2. For this reason, I do not think that the out-of-sample algorithm described in Algorithm 3 is correct. The conditional distribution for the missing observations $s_e$,$r_e$ and $t_e$ should depend on ($s_{e'}$ ; $r_{e'}$ ; $t_{e'}$ )$_{e': t_e < t_{e'} < t_{e} + l_e }$, which is not what is done. }}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor... \\

\noindent  {\bf E8} \grey{\emph{The authors should provide more details on the Metropolis-Hastings
proposals for the parameters $b$ and $\eta$ in Section 3.2. }}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor... \\


\noindent  {\bf E9} \grey{\emph{The sampler usePs a blocked Gibbs sampler for the $u_{iej}$. Given the constraint that $\sum_j u_{iej} > 0$, this sampler may be rather inefficient in the case where there are many one-to-one interactions (as is the case in the application, where 83\% of the interactions are dyadic). In order to go from the state $u_{ie}$ = (1,0,0,0,0,0) to $u_{ie}$ = (0,1,0,0,0,0) one needs to go through a state where two receivers are activated, which has low probability in this case. It would be good to comment on this, and in general on the mixing of the MCMC sampler. }}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor... \\


\noindent  {\bf E10} \grey{\emph{The authors provide in Section 3.2. some sanity checks on the sampler. While this is good practice to perform such checks, I am not sure it is very useful to include this in the main body (could be moved to the appendix), as this is done on a very small scale example with 5 nodes and 100 events, and does not really give an indication on the convergence properties of the algorithm in a more realistic scenario. I suggest the authors perform a simulation study with a larger number of nodes and events to demonstrate that the algorithm is able to approximate the posterior distribution well in that case.
}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor... \\



\noindent  {\bf E11} \grey{\emph{The authors should provide some indication of the computational complexity per iterations of their sampler. The application to email interaction data is rather small (18 nodes and 680 emails), so it would be good to know how many nodes/events the proposed approach can handle.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor... \\



\noindent  {\bf E12} \grey{\emph{Typos: The article contains numerous typos. Here are some of them:}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The Editor... \\



\noindent \underline{\textbf{Response to Comments by the Reviewer}}\\

\noindent {\bf R1} \grey{\emph{ TYPOS...}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\

\noindent {\bf R2} \grey{\emph{UNCLEAR PARTS...}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\noindent {\bf R3} \grey{\emph{BIBLIOGRAPHY:...}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\noindent {\bf R4} \grey{\emph{As already mentioned, the authors focus on the somewhat less explored area of continuous?time relational event models where each edge has its own different time index?instead of considering time?varying models for snap- shots of networks collected on a pre?specified time grid. However, the contribution is still within the general class of dynamic network inference. In this respect, the literature review provides a poor picture for the state-of-the-art in this wider framework. I think the authors should provide a more comprehensive literature review including also temporal ERGMs and dynamic latent variable models (e.g. dynamic stochastic block models, dynamic mixed membership stochastic block models, dynamic latent space models, . . .). Discussing your contribution in the light of these alternative (and quite different) models would further clarify the key novelties our the proposed methods.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\

\noindent {\bf R5} \grey{\emph{I fully understood the first paragraph in page 3 (summarizing HEMs) after reading the subsections 2.1, 2.2 and 2.3. This part should provide a much clear picture of your model instead of creating confusion. I suggest to improve it, leveraging also some intuitive illustrative figure. For example you could place Figure 1 much early and comment it while summarizing the HEM at the beginning of Section 2.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\noindent {\bf R6} \grey{\emph{You present the full conditionals in page 7 assuming uninformative $N(0, \infty)$ priors, but then you rely on weakly informative priors in the application (see page 17). I found this confusing. I?d present results in page 7 for generic Gaussian priors.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\noindent {\bf R7} \grey{\emph{Given that the model is relatively complex, some sensitivity analyses should be carried out to check how much posterior inference is affected by the hyperparameters? settings, and, possibly, suggest some default values.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\noindent {\bf R8} \grey{\emph{There are many MH routines in the literature. Which type of MH do you consider? What is the proposal distribution? What about the acceptance rate? Is there any smart proposal in this case which helps in increasing the acceptance rate.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\noindent {\bf R9} \grey{\emph{You rely on data augmentation MCMC, which has been shown to mix quite poorly (also in recent theoretical papers). Indeed, as expected, you end up thinning the chains every 40 samples in the application. However, there is no comment on this in the paper. I think it should be highlighted and not just hidden in the thinning.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\noindent {\bf R9} \grey{\emph{Your quantitative assessments are based on 5 nodes in the simulation and 18 nodes in the application. These are quite small networks compared to those one would expect in real?world settings (such as those you list in the introduction). To what extent are your computational methods able to scale to much larger networks? An application to a bigger dataset would be useful. Moreover, more information on computational time should be provided.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\noindent {\bf R10} \grey{\emph{You compare performance in predicting missing senders with random guess 1/18. This is a quite naive competitor and I am sure the authors can find much better ones?still relatively simple.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\

\noindent {\bf R11} \grey{\emph{In Figure 4c the choice of the logarithmic scale for MdAPE seems to hide a quite poor performance of your model in predicting the timestamps. It is true that the log?normal improves over the exponential, but the log?normal boxplot has still a third quartile providing an MdAPE of exp(7.36) which looks quite big. This is to me a negative result, which should be discussed and addressed in the paper.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\



\noindent {\bf R12} \grey{\emph{Based on your comments in the paper you are interpreting the posterior expectations of exp($\eta_6$) and exp($\eta_7$)--i.e. E[exp($\eta_6$) | data] and E[exp($\eta_7$) | data]. However you seem to compute this as exp[E(exp($\eta_6$) | data)] = exp(1.552) and exp[E(exp($\eta_7$) | data)] = exp(0.980). By Jensen inequality this is wrong.}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\

\noindent {\bf R13} \grey{\emph{It is also not correct to claim that the $e^{th}$ email is expected to take E[exp($\eta$) | data] longer compared to their counterpart. The term longer seems to refer to an additive effect on the event timing, which is not the case here since you assume a log?normal for the event timing and hence the effect is on the scale of the timing and not on the location?note that, if $X \sim \text{log-normal}(\mu, \sigma^2)$, then $E(X) = exp(\mu + \sigma^2/2)$. The authors should be more careful in the interpretations on the original time scale, or, more conservatively, they could simply comment on the effects on the log-time (as done immediately after).}}\\

\noindent \textcolor{MyGreen}{\textbf{Addressed:}} The reviewer... \\


\end{document}

