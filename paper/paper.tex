\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage[algo2e]{algorithm2e}
\usepackage{algorithmic}  
\usepackage{algorithm}
\usepackage[toc,page]{appendix}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}
\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf A Continuous-time Multicast Network Model}
  \author{Bomin Kim\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of Statistics, Pennsylvania State University,\\
  Aaron Schein\\
  College of Information and Computer Sciences, UMass Amherst,\\
    Bruce Desmarais\\
    Department of Political Science, Pennsylvania State University,\\
    and\\
    Hanna Wallach\\
    Microsoft Research NYC}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf A Continuous-time Multicast Network Model}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
We introduce the continuous-time multicast network model---a generative model for directed edges with multiple recipients or multiple senders. To define the model, we
integrate a dynamic version of the exponential random graph model (ERGM) and generalized linear model approach to understand who communicates with whom, and when. We use
the model to analyze emails sent between department managers in Montgomery county government in North Carolina, and international sanctions data concerning the threats and use of economic sanctions by states. Our applications demonstrate that the model is effective at predicting and explaining continuous-time networks involving edges with multiple recipients or senders.
	\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}\label{sec:intro}
\section{A Continuous-time Multicast Network Model}\label{sec:generative process}
Data generated under the model consists of $D$ unique edges. A single edge, indexed by $d \in [D]$, is represented by the three components: the sender $a_d \in [A]$, an indicator vector of recipients $\boldsymbol{r}_d = \{u_{dr} \}_{r=1}^{A}$, and the timestamp $t_d \in (0, \infty)$. For simplicity, we assume that edges are ordered by time such that $t_d < t_{d+1}$.
\subsection{Tie Generating Process}\label{subsec: Tie}
For every possible author--recipient pair $(a,r)_{a \neq r}$, we define the ``recipient intensity", which is the likelihood of edge $d$ being sent from $a$ to $r$:
\begin{equation}
\lambda_{adr} = {\boldsymbol{b}}^{\top}\boldsymbol{x}_{adr},
\end{equation}
where $\boldsymbol{b}$ is $P$--dimensional vector of coefficients and $\boldsymbol{x}_{adr}$ is a set of network features which vary depending on the hypotheses regarding canonical processes relevant to network theory such as popularity, reciprocity, and transitivity. We place a Normal prior $\boldsymbol{b} \sim N(\boldsymbol{\mu}_b, \Sigma_b)$.

Next, we hypothesize ``If $a$ were the sender of edge $d$, who would be the recipient(s)?" To do this, we draw each sender's set of recipients from a non-empty Gibbs measure \cite{fellows2017removing}---a probability measure we defined in order to 1) allow multiple recipients or ``multicast", 2) prevent from obtaining zero recipient, and 3) ensure tractable normalizing constant. Because we allows multicast, we draw a binary (0/1) vector $\boldsymbol{u}_{ad}= (u_{ad1},
\ldots, u_{adA})$
\begin{equation} \boldsymbol{u}_{ad}  \sim
\mbox{Gibbs}(\delta, \boldsymbol{\lambda}_{ad}),
\end{equation}
where $\delta$ is a real number controlling the average number of recipients and $\boldsymbol{\lambda}_{id}= \{\lambda_{adr}\}_{r=1}^A$. We place a Normal prior $\delta \sim N(\mu_\delta,\sigma^2_\delta)$. In particular, we define $\mbox{Gibbs}(\delta, \boldsymbol{\lambda}_{ad})$ as
\begin{equation}
\begin{aligned}
&p(\boldsymbol{u}_{ad}|\delta, \boldsymbol{\lambda}_{ad}) = \frac{\exp\Big\{\mbox{log}\big(\text{I}( \lVert \boldsymbol{u}_{ad}\rVert_1 > 0 )\big) + \sum_{r\neq a} (\delta+\lambda_{adr})u_{adr}\Big\}}{Z(\delta,\boldsymbol{\lambda}_{ad})} ,
\end{aligned}
\label{eqn:Gibbs}
\end{equation}
where $Z(\delta,\boldsymbol{\lambda}_{ad})= \prod_{r \neq a} (\mbox{exp}\{\delta+\lambda_{adr}\} + 1)-1$ is the normalizing constant and $\lVert \cdot \rVert_1$ is the $l_1$--norm. We provide the derivation of the normalizing constant as a tractable form in the supplementary material. 


\subsection{Time Generating Process}\label{subsec:Time}
Similarly, we hypothesize ``If $a$ were the sender of edge $d$, when would it be sent?" and define the ``timing rate" for sender $a$
\begin{equation}
\mu_{ad} = g^{-1}(\boldsymbol{\eta}^\top \boldsymbol{y}_{ad}),
\end{equation}
where $\boldsymbol{\eta}$ is $Q$--dimensional vector of coefficients with a Normal prior $\boldsymbol{\eta} \sim N(\boldsymbol{\mu}_\eta,\Sigma_\eta)$, $\boldsymbol{y}_{ad}$ is a set of time-related covariates, e.g., any feature that could affect timestamps of the edge, and $g(\cdot)$ is the appropriate link function such as identity, log, or inverse. 

In modeling ``when", we do not directly model the timestamp $t_d$. Instead, we assume that each sender's the time-increment or ``time to next edge" (i.e., $\tau_{d} = t_d-t_{d-1}$) is drawn from a specific distribution in the exponential family.  We follow the generalized linear model framework:
\begin{equation}
\begin{aligned}
E(\tau_{ad}) &= \mu_{ad},\\
V(\tau_{ad}) &= V(\mu_{ad}),
\end{aligned}
\end{equation}
where $\tau_{ad}$ is a positive real number. Possible choices of distribution include Exponential, Weibull, Gamma, and lognormal\footnote{lognormal distribution is not exponential family but can be used via modeling of $\log(\tau_d)$.} distributions, which are commonly used in time-to-event modeling. Based on the choice of distribution, we may introduce any additional parameter (e.g., $\sigma_\tau^2$) to account for the variance. We use $\phi_\tau(\cdot; \mu, \sigma_\tau^2)$ and $\Phi_\tau(\cdot; \mu, \sigma_\tau^2)$ to denote the probability density function (p.d.f) and cumulative density function (c.d.f), respectively, with mean $\mu$ and variance $\sigma^2$.

\subsection{Observed Data}\label{subsec:Observed}
Finally, we choose the sender, recipients, and timestamp---which will be observed---by selecting the sender--recipient-set pair with the smallest time-increment \cite{snijders1996stochastic}:
\begin{equation}
\begin{aligned}
a_d &= \mbox{argmin}_{a}(\tau_{ad}),\\
\boldsymbol{r}_d &= \boldsymbol{u}_{a_d d},\\
t_d &=t_{d-1} + \tau_{a_d d}.
\end{aligned}
\end{equation}
Therefore, it is a sender-driven process in that the recipients and timestamp of an edge is determined by the sender's urgency to send the edge to chosen recipients. 
	\begin{algorithm}[H]
		\spacingset{1}
		\SetAlgoLined
		\caption{Generating Process}
		\For{d=1 to D}{
			\For{a=1 to $A$}{
				\For{r=1 to $A$}{
					\If {r $\neq$ a} {
						form $\boldsymbol{x}_{adr}$\\
						set $\lambda_{adr} = {\boldsymbol{b}}^{\top}\boldsymbol{x}_{adr}$
					}  }
					draw $\boldsymbol{u}_{ad}  \sim
					\mbox{Gibbs}(\delta, \boldsymbol{\lambda}_{ad})$\\
					form $\boldsymbol{y}_{ad}$\\
					set $\mu_{ad} = g^{-1}(\boldsymbol{\eta}^\top \boldsymbol{y}_{ad})$\\
					draw $\tau_{ad} \sim \phi_\tau(\mu_{ad}, \sigma_\tau^2)$
				}
				set $a_d= \mbox{argmin}_{a}(\tau_{ad})$\\
				set $\boldsymbol{r}_d = \boldsymbol{u}_{a_d d}$\\
				set $t_d =t_{d-1} + \min_a\tau_{ad}$
			}
			\label{alg:generative}
		\end{algorithm}
\spacingset{1.45}
\section{Posterior Inference}\label{sec:inference}
Our inference goal is to invert the generative process to obtain the posterior distribution over the unknown parameters, conditioned on the observed data and hyperparamters $(\boldsymbol{\mu}_b, \Sigma_b, \boldsymbol{\mu}_\eta, \Sigma_\eta, {\mu}_\delta,\sigma^2_\delta)$. We draw the samples using Markov chain Monte Carlo (MCMC) methods, repeatedly resampling the value of each parameter from its conditional posterior given the observed data, hyperparamters, and the current values of the other parameters. We express each parameter’s conditional posterior in a closed form using the data augmentation schemes in $\boldsymbol{u}$ \cite{tanner1987calculation}. In this section, we provide each latent variable's conditional posterior. \\ \newline
First, since $u_{adr}$ is a binary random variable, new values may be sampled directly using
\begin{equation}
\begin{aligned}
&P(u_{adr}=1| \boldsymbol{u}_{ad\backslash r}, \boldsymbol{b}, \delta, \boldsymbol{x})
\propto \mbox{exp}\{\delta+\lambda_{adr}\};\\
&P(u_{adr}=0| \boldsymbol{u}_{ad\backslash r},\boldsymbol{b}, \delta, \boldsymbol{x})\propto \text{I}(\lVert\boldsymbol{u}_{ad\backslash r}\rVert_1 > 0 ),
\end{aligned}
\label{eqn:latentreceiver}
\end{equation}
where $I(\cdot)$ is the indicator function that is used to prevent from the instances where a sender chooses zero number of recipients.\\ \newline
New values for continuous variables $\delta, \boldsymbol{b},$ and $\boldsymbol{\eta}$ and $\sigma^2_\tau$ (if applicable) cannot be sampled directly from their conditional posteriors, but may instead be obtained using the Metropolis--Hastings algorithm. With uninformative priors (i.e., $N({0},\infty)$), the conditional posterior over $\delta$ and $\boldsymbol{b}$ is
\begin{equation}
P(\boldsymbol{b}, \delta| \boldsymbol{u}, \boldsymbol{x})\propto \prod_{d=1}^D
\prod_{a=1}^A \frac{\exp\Big\{\mbox{log}\big(\text{I}( \lVert \boldsymbol{u}_{ad}\rVert_1 > 0)\big) + \sum\limits_{r \neq a} (\delta+\lambda_{adr})u_{adr}\Big\}}{Z(\delta,\boldsymbol{\lambda}_{ad})},
\end{equation}
where the two variables share the conditional posterior and thus can be jointly sampled. Likewise, assuming uninformative priors on $\boldsymbol{\eta}$ (i.e., $N({0},\infty)$) and $\sigma_{\tau}^2$ (i.e., half-Cauchy($\infty$)), the conditional posterior is
\begin{equation}
P(\boldsymbol{\eta}, \sigma_\tau^2| \boldsymbol{u}, \boldsymbol{y})\propto \prod_{d=1}^D\Big(\phi_{\tau}(\tau_{d}; \mu_{a_d d}, \sigma_\tau^2)\times \prod_{a\neq a_d}\big(1-\Phi_{\tau}(\tau_{d}; \mu_{a d}, \sigma_\tau^2) \big)\Big).
\end{equation}

 \begin{algorithm}[H]
 			\spacingset{1}
 	\SetAlgoLined
 	\caption{MCMC Algorithm}
 	set initial values of $\boldsymbol{b}, \delta, \boldsymbol{\eta}, \sigma_\tau^2$\\
 	\For{o=1 to outer}{
 	 		\For{d=1 to D}{
 			\For{a = 1 to A}{
 				\For{r = a to A}{
 					\If {r $\neq$ a} {
 						update $u_{adr}$ using Gibbs update --- Equation (7)
 					}
	 			}
 			}
 		 		\For{n=1 to inner1}{
 		 			update $\boldsymbol{b}$ and $\delta$ using Metropolis-Hastings --- Equation (8)
 				}
 		\For{n=1 to inner2}{
 				update $\boldsymbol{\eta}$ and $\sigma_\tau^2$ using Metropolis-Hastings --- Equation (9)
 		}
 			\For{n=1 to inner3}{
 				update $\sigma_\tau^2$ using Metropolis-Hastings --- Equation (9)
 			}
 	}
 }
 	Summarize the results with: \\last chain of $\boldsymbol{b}$ and $\delta$, and last chain of $\boldsymbol{\eta}$ and $\sigma_\tau^2$ 
 \end{algorithm}
\section{Case Studies}\label{sec:case studies}
We now present two case studies applying our method to 1) county government emails (single sender multiple recipients) and 2) international sanctions (multiple senders single recipient). For each case study,
we formulate the network statistics $\boldsymbol{x}$ and timestamp statistics $\boldsymbol{y}$ and ground them in
illustrative examples. We then report a suite of experiments that test our method’s ability to form the posterior distribution over latent variables for different types of data. 
 	   \subsection{Case Study 1: County Government Emails}\label{subsec:Emails}
 	  	   \subsubsection{Montgomary County Data}\label{subsubsec:Montgomery}
 	   Our data come from the North Carolina county government email dataset collected by \cite{ben2017transparency} that includes internal email corpora covering the inboxes and outboxes of managerial-level employees of North Carolina county governments. Out of over twenty counties, we chose Montgomery County to 1) test our model using data with a lot of multicast edges (16.76\%), and 2) limit the scope of this initial application. The Montgomery County email network contains 680 emails, sent and received by 18 department managers over a period of 3 months (March--May) in 2012. 
 	   \subsubsection{Covariates}\label{subsec:Covariates_email}
In the example of email networks, we form the covariate vector $\boldsymbol{x}_{adr}$ using dynamic network statistics on three time intervals prior to and including $t_{d-1}$. We compute eight network statistics within each time interval \cite{PerryWolfe2012}, where the intervals are $(t_{d-1}-384h, t_{d-1}-96h], (t_{d-1}-96h, t_{d-1}-24h]$ and $(t_{d-1}-24h, t_{d-1}]$. We define the intervals to have equal length in the log-scale, and use $i=1$ to denote the earliest interval---i.e., $(t_{d-1}-384h, t_{d-1}-96h]$---and i = 3 to denote the latest. 
\begin{itemize}
	 			\spacingset{1}
	\item[1.] outdegree$(a,i)=\sum\limits_{d^\prime:t_{d^\prime \in i}} I(a_{d^\prime} = a)$;
	\item[2.] indegree$(r,i)=\sum\limits_{d^\prime:t_{d^\prime \in i}} I(u_{d^\prime r} = 1)$;
	\item[3.] send$(a,r,i)=\sum\limits_{d^\prime:t_{d^\prime \in i}} I(a_{d^\prime} = a)I(u_{d^\prime r} = 1)$;
	\item[4.] receive$(a,r,i)=\mbox{send}(r,a,i)$;
	\item[5.] 2-send$(a,r,i) = \sum\limits_{\substack{i^\prime, i^{\prime\prime}\geq i:\\i^\prime = i \mbox{ \small or }i^{\prime\prime}=i}}\sum\limits_{h \neq a, r} \mbox{send}(a,h,i^\prime)\mbox{send}(h,r,i^{\prime\prime})$;
	\item[6.] 2-receive$(a,r,i) = \sum\limits_{\substack{i^\prime, i^{\prime\prime}\geq i:\\i^\prime = i \mbox{ \small or }i^{\prime\prime}=i}}\sum\limits_{h \neq a, r} \mbox{send}(h,a,i^\prime)\mbox{send}(r,h,i^{\prime\prime})$;
	\item[6.] sibling$(a,r,i) =\sum\limits_{\substack{i^\prime, i^{\prime\prime}\geq i:\\i^\prime = i \mbox{ \small or }i^{\prime\prime}=i}}\sum\limits_{h \neq a, r} \mbox{send}(h,a,i^\prime)\mbox{send}(h,r,i^{\prime\prime})$;
	\item[6.] cosibling$(a,r,i) =\sum\limits_{\substack{i^\prime, i^{\prime\prime}\geq i:\\i^\prime = i \mbox{ \small or }i^{\prime\prime}=i}}\sum\limits_{h \neq a, r} \mbox{send}(a,h,i^\prime)\mbox{send}(r,h,i^{\prime\prime})$;
\end{itemize}
where $I(\cdot)$ is an indicator function. Note that in order to obtain two-path statistics (i.e., 2-send, 2-receive, sibling, and cosibling) within a single time interval $i$, we compute the number of two-paths from $a$ to $r$ by summing over the pairs of intervals $(i^\prime, i^{\prime\prime})$ where the earlier email in the path was sent during interval $i$. 

For time-related covariates $\boldsymbol{y}_{ad}$, 

\subsubsection{Experiment}\label{subsubsec:Experiment_email}
\subsubsection{Posterior Predictive Checks}\label{subsubsec:PPC_email} 	   
 	    	   Finally, we perform posterior predictive checks \cite{rubin1984bayesianly} to evaluate the appropriateness of our model specification for the Montgomery County email data. We formally generated entirely new data, by simulating ties and timestamps $\{(a_{d}, \boldsymbol{r}_{d}, t_{d})\}_{d=1}^D$ from the genenerative process in Section \ref{sec:generative process}, conditional upon a set of inferred parameter values from the inference in Section \ref{sec:inference}. For the test of goodness-of-fit in terms of network dynamics, we use multiple network statistics that summarize meaningful aspects of the Montgomery County email network: indegree distribution, outdegree distribution, recipient size distribution, time-increments distribution, the edgewise shared partner distribution, and the geodesic distance distribution. We then generated 500 synthetic networks from the posterior predictive distribution.
 	    	   \begin{algorithm}[H]
 	    	   	\spacingset{1}
 	    	   	\caption{Generate new data for PPC}
 	    	   	\begin{algorithmic}
 	    	   		\STATE \textbf{Input}: number of new data to generate $R$,\\
 	    	   		estimated latent variables $(\boldsymbol{u}, \boldsymbol{b}, \delta, \boldsymbol{\eta},  \sigma_\tau^2)$\\
 	    	   		\vskip 0.1in
 	    	   		
 	    	   		\FOR{$r=1$ {\bfseries to}  $R$}
 	    	   		\FOR{$d = 1$ {\bfseries to}  $D$} 
 	    	   		\STATE  Compute $\boldsymbol{x}_{d}$ and $\boldsymbol{y}_{d}$ given $\{(a_{d}, \boldsymbol{r}_{d}, t_{d})\}_{[1:(d-1)]}$
 	    	   		\STATE	Draw ($a_{d}$, $\boldsymbol{r}_{d}$, $t_{d}$) following Section \ref{sec:generative process}\\
 	    	   		\ENDFOR
 	    	   		\STATE Store every $r^{th}$ new data $\{(a_{d}, \boldsymbol{r}_{d}, t_{d})\}_{d=1}^D$ 
 	    	   		\ENDFOR
 	    	   	\end{algorithmic}
 	    	   \end{algorithm}       
\subsection{Case Study 2: International Sanctions}\label{subsec:International Sanctions}
\subsubsection{Threat and Imposition of Sanctions (TIES) Data}\label{subsubsec:TIES}
We processed the threat and imposition of sanctions (TIES) data \citep{morgan2014threat} that includes the records of international sanctions---defined as actions that one or more countries take to limit or end their economic relations with a target country in an effort to persuade that country to change one or more of its policies---from 1945 to 2005.
\subsubsection{Covariates}\label{subsec:Covariates_sanction}
\subsubsection{Experiment}\label{subsec:Experiment_sanction}
\subsubsection{Posterior Predictive Checks}\label{subsubsec:PPC_sanction} 	   
\section{Conclusion}\label{sec:conclusion}
\newpage
\bibliographystyle{agsm}
\bibliography{multicastnetwork}
\begin{appendices}
\section{Normalizing constant of Gibbs measure}\label{sec: non-empty Gibbs measure}
The non-empty Gibbs measure \cite{fellows2017removing} defines the probability of author $a$ selecting the binary recipient vector $\boldsymbol{u}_{ad}$ as
\begin{equation*} 
\begin{aligned}
& P(\boldsymbol{u}_{ad}| \delta, \boldsymbol{\lambda}_{ad} ) = \frac{\exp\Big\{ \mbox{log}\big(\text{I}(\lVert \boldsymbol{u}_{ad} \rVert_1 > 0)\big) + \sum_{r \neq a} (\delta+\lambda_{adr})u_{adr} \Big\}}{Z(\delta,\boldsymbol{\lambda}_{ad})}.
\end{aligned}
\end{equation*}

To use this distribution efficiently, we derive a closed-form expression for $Z(\delta,\boldsymbol{\lambda}_{id})$ that does not require brute-force summation over the support of $\boldsymbol{u}_{ad}$ (\textit{i.e.} $\forall \boldsymbol{u}_{ad} \in [0,1]^A$). We recognize that if $\boldsymbol{u}_{ad}$ were drawn via independent Bernoulli distributions in which $P({u}_{adr}=1|\delta, \boldsymbol{\lambda}_{ad})$ was given by logit$(\delta+\lambda_{adr})$, then 
\begin{equation*}
P(\boldsymbol{u}_{ad}|\delta, \boldsymbol{\lambda}_{ad}) \propto \exp\Big\{\sum_{r \neq a } (\delta+\lambda_{adr})u_{adr}\Big\}.  	 
\end{equation*}
This is straightforward to verify by looking at 
\begin{equation*}
\begin{aligned}
&P(u_{adr}=1|\boldsymbol{u}_{ad[-r]}, \delta, \boldsymbol{\lambda}_{ad})
=\frac{ \exp{(\delta+\lambda_{adr})}}{\exp{(\delta+\lambda_{adr})} + 1}.\end{aligned}\end{equation*}
We denote the logistic-Bernoulli normalizing constant as $Z^{l}(\delta,\boldsymbol{\lambda}_{ad})$, which is defined as 
\begin{equation*}
Z^{l}(\delta,\boldsymbol{\lambda}_{ad})=\sum_{\boldsymbol{u}_{ad} \in [0,1]^{A}} \exp\Big\{\sum_{r\neq a} (\delta+\lambda_{adr})u_{adr}\Big\}.
\end{equation*}
Now, since 
\begin{equation*}
\begin{aligned}
&\exp\Big\{ \mbox{log}\Big(\text{I}(\lVert \boldsymbol{u}_{ad} \rVert_1 > 0)\Big) + \sum_{r \neq a} (\delta+\lambda_{adr})u_{adr} \Big\}\\&= \exp\Big\{  \sum_{r \neq a} (\delta+\lambda_{adr})u_{adr} \Big\},
\end{aligned}
\end{equation*}
except when $\lVert \boldsymbol{u}_{ad} \rVert_1=0$, we note that 
\begin{equation*}
\begin{aligned}
Z(\delta,\boldsymbol{\lambda}_{ad})& = Z^{l}(\delta,\boldsymbol{\lambda}_{ad}) -\exp\Big\{ \sum\limits_{\forall u_{adr}=0}(\delta+\lambda_{adr})u_{adr} \Big\}
\\& = Z^{l}(\delta,\lambda_{a}^{(d)}) -  1.
\end{aligned}
\end{equation*}
We can therefore derive a closed form expression for $Z(\delta,\boldsymbol{\lambda}_{ad})$ via a closed form expression for $Z^{l}(\delta,\boldsymbol{\lambda}_{ad})$. This can be done by looking at the probability of the zero vector under the logistic-Bernoulli model:
\begin{equation*}
\begin{aligned}
&\frac{\exp\Big\{ \sum\limits_{\forall u_{adr}=0}(\delta+\lambda_{adr})u_{adr} \Big\}}{Z^{l}(\delta,\boldsymbol{\lambda}_{ad})}= \prod_{r \neq a}   \Big(1-\frac{ \exp{(\delta+\lambda_{adr})}}{\exp{(\delta+\lambda_{adr})} + 1}\Big).
\end{aligned}  
\end{equation*}
Then, we have 
\begin{equation*}
\begin{aligned}
& \frac{1}{Z^{l}(\delta,\boldsymbol{\lambda}_{ad})} &= \prod\limits_{r \neq a}\frac{1}{ \exp(\delta+\lambda_{adr})+ 1}.
\end{aligned}  
\end{equation*}
Finally, the closed form expression for the normalizing constant under the non-empty Gibbs measure is  \begin{equation*}
\begin{aligned}Z(\delta,\boldsymbol{\lambda}_{ad}) = \prod_{r \neq a } \big(\mbox{exp}\{\delta+\lambda_{adr}\} + 1\big)-1.
\end{aligned}  
\end{equation*}
 
	\end{appendices}
\end{document}